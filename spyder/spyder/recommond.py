import requests,json,timeimport re,stat,lxml.htmlfrom bs4 import BeautifulSoupfrom lxml import etreeimport datetimeimport timeimport datetimedef youhui():    res=requests.get(        url='http://www.zdzdm.com/youhui',        headers={            'User-Agent': 'Mozilla/5.0(Macintosh;Intel Mac 05 X 10_11_4)AppleWebKit/537.36(KHTML,like Gecko)Chrome/52.0.2743.116 Safari/537.36'})    # print(re.text)    soup=BeautifulSoup(res.text,'html.parser')    # div=soup.find(name="div",class_="mb_pages").find_all(name="div",class_="new_info_list")    list=soup.find_all(name="div",class_="new_info_list")    # print(div)    for item in list:        ahref=item.find(name="a",target="_blank")        # print(ahref,type(ahref))        # href=ahref.get("href")        # title=ahref.get("title")        # print(href,title)        # spanprice=item.find(name="a",target="_blank").select("span")        # text=spanprice[0].text        # price=re.search('([1-9].*)元',text)[0]        # print(price)        # img=item.find(name="div",class_="zdm_img").select("a img")        # imgurl=img[0].get("src")        # print(imgurl)        # ddescribe=item.find(name="div",class_="list_txt").select("p")        # describe=ddescribe[0].text        # print(describe)def gouwudanglishi():    res=requests.get(        url='https://www.gwdang.com/promotion/price',        headers={            'User-Agent': 'Mozilla/5.0(Macintosh;Intel Mac 05 X 10_11_4)AppleWebKit/537.36(KHTML,like Gecko)Chrome/52.0.2743.116 Safari/537.36'})    # print(res.text)    soup=BeautifulSoup(res.text,'html.parser')    list=soup.find_all(name="li",class_="zdm_li pos_r")    # print(list)    for item in list:        img=item.find(name="div",class_="section-1").select("a img")[0]        imgurl=img.get("data-original")        ahref = item.find(name="div", class_="section-2").select("div a")[0]        ohref=ahref.get("href")        href="https://www.gwdang.com/"+ohref        title=ahref.get("title")        price=item.find(name="div", class_="section-2").find(name="span",class_="price")        # print(price.text)        shop=item.find(name="div",class_="site-info pos_a").find(name="span",class_="site-name").text        # print(shop)        # print(href,title,imgurl)def zhidemai():    import stat    res = requests.get(        url='https://www.gwdang.com/promotion/zhi',        headers={            'User-Agent': 'Mozilla/5.0(Macintosh;Intel Mac 05 X 10_11_4)AppleWebKit/537.36(KHTML,like Gecko)Chrome/52.0.2743.116 Safari/537.36'})    # print(res.text)    soup = BeautifulSoup(res.text, 'html.parser')    list = soup.find_all(name="li", class_="zdm_li pos_r")    for item in list:        img = item.find(name="div", class_="section-1").select("a img")[0]        imgurl = img.get("data-original")        ahref = item.find(name="div", class_="section-2").select("div a")[0]        ohref = ahref.get("href")        href = "https://www.gwdang.com/" + ohref        title = ahref.get("title")        price = item.find(name="div", class_="section-2").find(name="span", class_="price").text        # print(price.text)        shop = item.find(name="div", class_="site-info pos_a").find(name="span", class_="site-name").text        # print(shop)        # print(href,title,imgurl,price,shop)        str=item.find(name="a",isconvert="1",class_="btn btn-buy pos_a").get("href")        if(str.startswith('/union/go')):            href="https://www.gwdang.com/"+str        else:            href=str        print(href)# 解析uuid的地址def max_min(url):    import re    res = requests.get(        url="https://www.gwdang.com/trend?url=" + url + "&days=60&crc64=1",        headers={            'User-Agent': 'Mozilla/5.0(Macintosh;Intel Mac 05 X 10_11_4)AppleWebKit/537.36(KHTML,like Gecko)Chrome/52.0.2743.116 Safari/537.36'})    soup = BeautifulSoup(res.text, 'lxml')    try :        # 通过收藏按钮找到uuid 然后去查找最高 最低价格        uuidstr=soup.find(name="div",class_="search_content").find(name="div",class_="collect").select("a")[0].get("url")        uuid = uuidstr.split("&", 3)[0]        print("uuid:",uuid)        maxminprice_list(uuid)    except Exception as e:        print("解析uuid出错")# 处理union的链接def unionhref(url):    res = requests.get(        url=url,        headers={            'User-Agent': 'Mozilla/5.0(Macintosh;Intel Mac 05 X 10_11_4)AppleWebKit/537.36(KHTML,like Gecko)Chrome/52.0.2743.116 Safari/537.36'        })    re.encoding = 'utf-8'    soup = BeautifulSoup(res.text, 'lxml')    href=soup.find(name="li",class_="gl-item").find(name="div",class_="p-name p-name-type-2").select("a")[0].get("href")    print("京东的链接：",href)    return(href)# 将整数转换成时间def dateparse(local):    tuptime = time.localtime(local)    standartime = time.strftime("%Y-%m-%d", tuptime)    return standartimemaxprice=Nonemaxdate=Nonemindate=Noneminprice=None# 得到最高最低价格的地址然后搜索def maxminprice_list(id):    res = requests.get(        url='https://www.gwdang.com/trend/data_www?'+id+'&show_prom=true&v=2&get_coupon=1&price=',        headers={            'authority': 'www.gwdang.com',            'method': 'GET',            'User-Agent': 'Mozilla/5.0(Macintosh;Intel Mac 05 X 10_11_4)AppleWebKit/537.36(KHTML,like Gecko)Chrome/52.0.2743.116 Safari/537.36'}    )    text=json.loads(res.text)    if(text.get("series")[0] is not None):        series=text.get("series")[0]#这是json转换过后的第一个dict        # print(series)        global maxdate        global maxprice        global mindate        global minprice        maxprice=series["max"]/100        maxprice_date=series["max_stamp"]        maxdate=dateparse(maxprice_date)        # print("最高价格：",maxprice/100,"最高价格日期：",maxdate)        minprice=series["min"]/100        minprice_date=series["min_stamp"]        mindate=dateparse(minprice_date)        # print("最低价格",minprice/100,"最低价格日期",mindate)# 查询所有的信息页面def hisprice(url):    res = requests.get(        url="https://www.gwdang.com/trend?url="+url+"&days=60&crc64=1",        headers={            'User-Agent': 'Mozilla/5.0(Macintosh;Intel Mac 05 X 10_11_4)AppleWebKit/537.36(KHTML,like Gecko)Chrome/52.0.2743.116 Safari/537.36'})    # print(res.text)    soup = BeautifulSoup(res.text, 'lxml')    list = soup.find_all(name="div", class_="b2c")    num=0    temp=None    before=None    for item in list:    # 找到便签查找个数        flag=item.find(name="img",class_="site-icon").get("alt")        # print(flag)        temp=flag        if(temp!=flag or num<3):            num=num+1            try:                href = item.find(name="div", class_="dp-img pic1").select("a")[0].get("href")                if (href.startswith('/union/go')):                    hrefs = "https://www.gwdang.com/" + href                    href = unionhref(hrefs)                max_min(href)            except Exception as e:                continue            try:                title = item.find(name="div", class_="dp-img pic1").select("a")[0].get("title")            except Exception as e:                continue            try:                imgurl = item.find(name="div", class_="dp-img pic1").select("a img")[0].get("data-original")            except Exception as e:                continue            try:                eva = item.find(name="div", class_="promos").select("span")[0].text            except Exception as e:                continue            try:                pricenow = item.find(name="div", class_="bottom").find(name="span", class_="price").text            except Exception as e:                continue            try:                shop = item.find(name="div", class_="product-site").select("span")[0].text            except Exception as e:                continue            print(flag, title, pricenow, maxprice,maxdate,minprice,mindate,eva, shop, href, imgurl)        else:            temp = None            num=0            continue        # print(flag,title,pricenow,eva,shop)# 得到jd的hrefdef getjdfirst(keys):    res = requests.get(        url="https://search.jd.com/Search?keyword=" + keys + "&enc=utf-8&",        headers={            'User-Agent': 'Mozilla/5.0(Macintosh;Intel Mac 05 X 10_11_4)AppleWebKit/537.36(KHTML,like Gecko)Chrome/52.0.2743.116 Safari/537.36'        })    re.encoding = 'utf-8'    soup = BeautifulSoup(res.text, 'lxml')    # uuid=soup.find(name="li",class_="gl-item").get("data-sku")    # print("第一次得到搜索的内容的jdsku", uuid)    href=soup.find(name="li",class_="gl-item").find(name="div",class_="p-name p-name-type-2").select("a")[0].get("href")    hrefs="http:"+href    print("京东的链接：",hrefs)    # hisprice(hrefs)# youhui()# gouwudanglishi()# zhidemai()hisprice("https://item.jd.com/100005171461.html")# pricelist("https://item.jd.hk/57334530295.html")# parses()# max_min("https://item.jd.com/4233197.html")# getjdfirst("iphone")# maxminprice_list("dp_id=564196183996-83")